# -*- coding: utf-8 -*-
"""rag_granite_llamaindex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nZXfAY9HYir3rHy9j1gmBtpvNNNB2dWG
"""

# Commented out IPython magic to ensure Python compatibility.
# # --- Environment Setup ---
# # Install required packages
# %%capture
# !pip install llama-index==0.10.65 --user
# !pip install llama-index-llms-ibm==0.1.0 --user
# !pip install llama-index-embeddings-ibm==0.1.0 --user
#

# --- Imports and Warning Suppression ---

# Suppress unwanted warnings
import warnings
def warn(*args, **kwargs):
    pass
warnings.warn = warn
warnings.filterwarnings("ignore")

# LlamaIndex + IBM watsonx.ai modules
from llama_index.llms.ibm import WatsonxLLM
from llama_index.embeddings.ibm import WatsonxEmbeddings
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter

# --- Load Private Document ---

# Example placeholder for a local private document
# Replace 'data/private_document.pdf' with your own file path
documents = SimpleDirectoryReader(input_files=["data/private_document.pdf"]).load_data()

print(f"Loaded {len(documents)} document(s) for processing.")

# --- Split Document into Nodes ---

# Split the document into smaller chunks (nodes) for embedding and retrieval
splitter = SentenceSplitter(chunk_size=500)
nodes = splitter.get_nodes_from_documents(documents)

print(f"Document split into {len(nodes)} nodes.")
print("Sample node content preview:\n")
print(nodes[0].get_content(metadata_mode=True)[:500])  # show first 500 characters for brevity

# --- Create Embeddings and Build Vector Index ---

# Initialize watsonx.ai embeddings model (replace placeholders with your credentials if running locally)
watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr-v2",
    url="your_watsonx_instance_url",
    project_id="your_project_id",
    truncate_input_tokens=3
)

# Build a vector index from the document nodes
index = VectorStoreIndex(
    nodes=nodes,
    embed_model=watsonx_embedding,
    show_progress=True
)

# Initialize retriever to fetch the top 3 most relevant nodes
retriever = index.as_retriever(similarity_top_k=3)

# Example query to test retrieval
query = "Explain the concept of GPT-2"
results = retriever.retrieve(query)

print(f"Query: {query}\n")
for node in results:
    print("-" * 60)
    print(f"Score: {node.score:.3f}")
    print(node.get_content()[:500])  # preview limited text
    print("-" * 60 + "\n")

# --- Initialize watsonx.ai LLM and Run Queries ---

# Define model generation parameters
temperature = 0.1
max_new_tokens = 75
additional_params = {
    "decoding_method": "sample",
    "min_new_tokens": 1,
    "top_k": 50,
    "top_p": 1
}

# Initialize watsonx.ai LLM (Granite model)
watsonx_llm = WatsonxLLM(
    model_id="ibm/granite-3-8b-instruct",
    url="your_watsonx_instance_url",
    project_id="your_project_id",
    temperature=temperature,
    max_new_tokens=max_new_tokens,
    additional_params=additional_params
)

# Quick model sanity check
test_prompt = "Explain the concept of Generative AI in simple terms."
test_response = watsonx_llm.complete(test_prompt)
print("Quick LLM Test Response:\n", test_response, "\n")

# Build a query engine using the existing index and watsonx LLM
query_engine = index.as_query_engine(
    streaming=False,
    similarity_top_k=5,
    llm=watsonx_llm
)

# Example queries for your private document
queries = [
    "Summarize the uploaded document.",
    "List the key insights or takeaways from the content.",
    "What topics or themes are discussed in this document?"
]

for q in queries:
    print(f"\nðŸ§  Query: {q}\n")
    response = query_engine.query(q)
    print(str(response))
    print("-" * 80)